\section{Analysis II - draft}

% GDG (1. Ordnung)___________________________________________________________________________________________________
%%%%%%
\subsection{7. GDG (1. Ordnung)}

\begin{itemize}
  \item gewöhnlich: \quad \( F(x, y(x), y'(x), y''(x), \dots) = 0 \)
  \item ordnung: \quad ordnung der höchsten ableitung
  \item linear: \quad \( p_n(x)y^{(n)} + \dots + p_1(x)y' + p_0(x)y + g(x) = 0 \)
  \item homogen: \quad \( p_n(x)y^{(n)} + \dots + p_1(x)y' + p_0(x)y = 0 \)
\end{itemize}

\textbf{Superpositionsprinzip:} \quad
jede lineare kombination von lösungen einer homogenen GDG ist ebenfalls lösung der GDG.

\textbf{Satz (Lösungsraum):}

\[
\dim \mathcal{Z} = n \quad / \quad \dim \mathcal{Z}_{\mathbb{R}} = n
\]

\begin{itemize}
  \item[(i)] die menge 
    \[
    \mathcal{Z} := \left\{ f \in \mathcal{C}^n(\mathbb{R}; \mathbb{C}) \;\middle|\; f \text{ löst } \sum_{i=0}^{n} a_i(x) f^{(i)}(x) = 0 \right\}
    \]
    ist ein komplexer vektorraum.
    
  \item[(ii)] die menge 
    \[
    \mathcal{Z} := \left\{ f \in \mathcal{C}^n(\mathbb{R}; \mathbb{R}) \;\middle|\; f \text{ löst } \sum_{i=0}^{n} a_i(x) f^{(i)}(x) = 0 \right\}
    \]
    ist ein reeller vektorraum.
\end{itemize}

\textbf{GDG der Form:} \quad
$f^{(n)} + a_{n-1} f^{(n-1)} + a_{n-2} f^{(n-2)} + \dots + a_0 f = 0$

\textbf{Ansatz:} \quad $f := e^{\lambda t}$

\textbf{Satz (Basis für den Lösungsraum):} \quad
die funktionen \( f_{ik}(t) := t^k e^{\lambda_i t} \), \quad \( 1 \leq i \leq \ell, \; 0 \leq k \leq m_i \) \\
bilden eine basis des komplexen vektorraums
\[
\mathcal{Z} := \left\{ f \in \mathcal{C}^n(\mathbb{R}; \mathbb{C}) \;\middle|\; f \text{ löst } \sum_{i=0}^{n} a_i f^{(i)} = 0 \right\}
\]

\textbf{Beispiel:} \quad
\[
\ddot{f} - 2\dot{f} + f = 0 \quad \Rightarrow \quad f(t) = c_1 e^t + c_2 t e^t
\]

\vspace{1em}

\textcolor{orange}{\textbf{Zweite Ordnung}} \quad
\textbf{GDG der Form:} \quad $\ddot{f} + a\_1 \dot{f} + a\_0 f = 0$ \qquad \text{(schwingkreis)}

\[
\omega_0 := \sqrt{a_0}, \qquad \delta := \frac{a_1}{2}
\quad \Rightarrow \quad
\ddot{f} + 2\delta \dot{f} + \omega_0^2 f = 0
\]

\[
\Rightarrow \quad \mu := \sqrt{\delta^2 - \omega_0^2}, \qquad \omega_d := \sqrt{\omega_0^2 - \delta^2}
\]

\begin{itemize}
  \item[1)] \( \delta < \omega_0 \): \quad
    \( f(t) = e^{-\delta t} \left( a \cos(\omega_d t) + b \sin(\omega_d t) \right) \)

  \item[2)] \( \delta = \omega_0 \): \quad
    \( f(t) = (c_1 + c_2 t) e^{-\delta t} \)

  \item[3)] \( \delta > \omega_0 \): \quad
    \( f(t) = c_1 e^{(-\delta + \mu)t} + c_2 e^{(-\delta - \mu)t} \)
\end{itemize}


% Differentialrechnung im R___________________________________________________________________________________________________
%%%%%%
\subsection{8. Differentialrechnung im R}

\textbf{Vektorwertige Ableitung:}

Sei $g = \begin{pmatrix} g_1 \\ \vdots \\ g_p \end{pmatrix} : U \to \mathbb{R}^p$ eine Funktion mit $U \subset \mathbb{R}$ offen. 

\vspace{2pt}

Falls $g_i$ in $y_0$ differenzierbar $\forall i$, dann ist $g$ in $y_0$ differenzierbar mit
\[
g'(y_0) := \begin{pmatrix} g_1'(y_0) \\ \vdots \\ g_p'(y_0) \end{pmatrix}.
\]

\textbf{Partielle Ableitung \& Jacobi-Matrix:}

Sei $f : U \subset \mathbb{R}^n \to \mathbb{R}^p$. Dann ist die \emph{partielle Ableitung} von $f$ nach $x^j$ an der Stelle $x_0$ definiert durch:
\[
f_{x^j}(x_0) := \frac{\partial f}{\partial x^j}(x_0) := g'(x_0^j),
\]
falls $g(y) := f(x_0^1, \dots, x_0^{j-1}, y, x_0^{j+1}, \dots, x_0^n)$ in $y = x_0^j$ differenzierbar ist.

\vspace{2pt}

Falls $f$ an $x_0$ nach allen Variablen partiell differenzierbar ist, ist die \textbf{Jacobi-Matrix} gegeben durch:
\[
J_f(x_0) := \begin{pmatrix}
f_{x^1}^1(x_0) & \cdots & f_{x^n}^1(x_0) \\
\vdots & \ddots & \vdots \\
f_{x^1}^p(x_0) & \cdots & f_{x^n}^p(x_0)
\end{pmatrix}.
\]

\textbf{Totale Differenzierbarkeit \& Jacobi-Matrix:}

$f : \mathbb{R}^n \to \mathbb{R}^p$ ist in $x_0$ \emph{differenzierbar} $\Leftrightarrow$ es existiert eine lineare Abbildung $A$ mit
\[
\lim_{x \to x_0} \frac{\|f(x) - f(x_0) - A(x - x_0)\|}{\|x - x_0\|} = 0.
\]
Dann ist $A = J_f(x_0)$ die \textbf{Jacobi-Matrix} von $f$ in $x_0$:
\[
A = J_f(x_0) : \mathbb{R}^n \to \mathbb{R}^p.
\]

\textbf{Folge:} Total differenzierbar $\Rightarrow$ partiell differenzierbar.

\textbf{Totale Ableitung:}  
$Df(x_0) := df(x_0)$ ist die beste lineare Approximation von $f$ in $x_0$  
\[
Df(x_0) = J_f(x_0) = \left( \frac{\partial f^i}{\partial x^j}(x_0) \right)
\]

\textbf{Kettenregel:} Wenn $f$ in $x_0$ und $g$ in $f(x_0)$ differenzierbar sind, dann ist auch $g \circ f$ in $x_0$ differenzierbar mit
\[
d(g \circ f)(x_0) = dg(f(x_0)) \circ (df(x_0)) = dg(f(x_0)) \cdot df(x_0)
\]

\textbf{Kettenregel (Jacobi-Matrizen):} Für differenzierbare $f,g$ gilt:
\[
J_{g \circ f}(x_0) = J_g(f(x_0)) \cdot J_f(x_0)
\]
Falls $n = p = q = 1$, dann ergibt das die klassische Kettenregel:
\[
(g \circ f)'(x_0) = g'(f(x_0)) f'(x_0)
\]

\textbf{Ableitungen: Summe, Skalarprodukt, Quotient}

Seien $f, g$ differenzierbar in $x_0$:

\begin{itemize}
  \item \textbf{Summe:} $d(f + g)(x_0) = df(x_0) + dg(x_0)$
  \item \textbf{Skalarprodukt:} $d(f \cdot g)(x_0) = g(x_0) \cdot df(x_0) + f(x_0) \cdot dg(x_0)$
  \item \textbf{Quotient (für $p=1$):}
  \[
  d\left(\frac{f}{g}\right)(x_0) = \frac{g(x_0)df(x_0) - f(x_0)dg(x_0)}{(g(x_0))^2}
  \]
\end{itemize}

\textbf{Richtungsableitung:} Sei $f : \mathbb{R}^n \to \mathbb{R}^p$, $x_0 \in \mathbb{R}^n$, $v \in \mathbb{R}^n$.\\
Falls die Funktion $g : \mathbb{R} \to \mathbb{R}^p$, definiert durch
\[
g(t) := f(x_0 + tv)
\]
im Punkt $t=0$ differenzierbar ist, so definieren wir die Richtungsableitung von $f$ an der Stelle $x_0$ in Richtung $v$ durch
\[
d_v f(x_0) := D_v f(x_0) := g'(0) = 
\begin{pmatrix}
g_1'(0) \\
\vdots \\
g_p'(0)
\end{pmatrix}
\in \mathbb{R}^p.
\]

\textbf{Gradient:} Für $f : \mathbb{R}^n \to \mathbb{R}$ ist der Gradient von $f$ an der Stelle $x$ der Vektor
\[
\nabla f(x) := 
\begin{pmatrix}
D_1 f(x) \\
\vdots \\
D_n f(x)
\end{pmatrix}
=
\begin{pmatrix}
f_{x^1}(x) \\
\vdots \\
f_{x^n}(x)
\end{pmatrix}.
\]

\textbf{Stetige partielle Differenzierbarkeit:}
$f \in C^1(U, \mathbb{R}^p)$ bedeutet: $f$ ist partiell differenzierbar und alle $\partial_j f$ sind stetig.

\textbf{Folge:} $f \in C^1(U, \mathbb{R}^p) \Rightarrow f$ ist überall total differenzierbar.

\textbf{Gradientenfeld und Potential:}

Ein \emph{Vektorfeld} ist eine Abbildung \( X : U \to \mathbb{R}^n \).

Sei \( f \in C^1(U) \), dann ist das \emph{Gradientenfeld} von \( f \) gegeben durch:
\[
\nabla f(x) := 
\begin{pmatrix}
D_1 f(x) \\
\vdots \\
D_n f(x)
\end{pmatrix}
= 
\begin{pmatrix}
f_{x^1}(x) \\
\vdots \\
f_{x^n}(x)
\end{pmatrix}
.
\]

\textbf{Beispiel:} \( f(x) := \|x\|^2 \Rightarrow \nabla f(x) = 2x \)

\textbf{Potential:} Ein \emph{Potential} für ein Vektorfeld \( X \) ist eine differenzierbare Funktion \( f : U \to \mathbb{R} \) mit:
\[
\nabla f = X.
\]

\textbf{Konservativ:} Das Vektorfeld \( X \) heisst \emph{konservativ}, falls es ein Potential \( f \) besitzt.

\textbf{Wegintegral:}

\textit{Definition:} Das Wegintegral eines Vektorfeldes \( X \) längs einer Kurve \( \gamma: [a, b] \to \mathbb{R}^n \) ist definiert durch:
\[
\int_\gamma X \cdot d\gamma := \int_a^b X(\gamma(t)) \cdot \dot{\gamma}(t) \, dt.
\]

\textbf{Beispiel:} Sei \( X(x, y) := (2x, 3y) \), und \(\gamma(t) := (t, t^2) \) für \( t \in [0, 1] \). Dann:
\[
X(\gamma(t)) = (2t, 3t^2), \quad \dot{\gamma}(t) = (1, 2t),
\]
\[
X(\gamma(t)) \cdot \dot{\gamma}(t) = 2t \cdot 1 + 3t^2 \cdot 2t = 2t + 6t^3,
\]
\[
\int_\gamma X \cdot d\gamma = \int_0^1 (2t + 6t^3) \, dt = \left[t^2 + \tfrac{3}{2}t^4\right]_0^1 = 1 + \tfrac{3}{2} = \tfrac{5}{2}.
\]


\textbf{Geschlossener Weg:} \\
Ein Weg \( \gamma : [a, b] \to U \) heisst \textbf{geschlossen}, falls \( \gamma(a) = \gamma(b) \).

\vspace{0.5em}
\textbf{Satz: Charakterisierung konservativer Felder} \\
Sei \( X : U \to \mathbb{R}^n \) stetig mit \( U \subset \mathbb{R}^n \) offen. Die folgenden Aussagen sind äquivalent:

\begin{itemize}
  \item[(a)] \( X \) ist konservativ.
  \item[(b)] Das Wegintegral hängt nur von Endpunkten ab: Wenn \(\gamma_0, \gamma_1: [a,b] \to U\) stückweise stetig differenzierbar mit
  \[
    \gamma_0(a) = \gamma_1(a), \quad \gamma_0(b) = \gamma_1(b),
  \]
  dann gilt
  \[
    \int X \cdot d\gamma_0 = \int X \cdot d\gamma_1.
  \]
  \item[(c)] Für jeden geschlossenen, stetig differenzierbaren Weg \( \gamma \) gilt:
  \[
    \int_\gamma X \cdot d\gamma = 0.
  \]
\end{itemize}


\textbf{Weg-Zusammenhang und Konvexität:}

\begin{itemize}
  \item \textbf{Weg-zusammenhängend:} \( S \subset \mathbb{R}^n \) heisst weg-zusammenhängend, wenn es für alle \( x_0, x_1 \in S \) einen stetigen Weg \( \gamma: [0,1] \to S \) mit
  \[
    \gamma(0) = x_0, \quad \gamma(1) = x_1
  \]
  gibt.
  
  \item \textbf{Konvex:} \( S \subset \mathbb{R}^n \) heisst konvex, wenn für alle \( x_0, x_1 \in S \) und \( t \in [0,1] \) gilt:
  \[
    (1 - t)x_0 + tx_1 \in S.
  \]
\end{itemize}

\textbf{Konservativität überprüfen:}

\begin{enumerate}
  \item Wähle \( x_0 \in U \) und für jedes \( x \in U \) einen Weg \( \gamma_x \) von \( x_0 \) nach \( x \).
  \item Definiere \( f(x) := \int_{\gamma_x} X \cdot d\gamma \).
  \item Berechne \( \nabla f \).
  \item Falls \( \nabla f = X \), ist \( X \) konservativ, \( f \) ist Potential.
  \item Falls \( \nabla f \neq X \), ist \( X \) nicht konservativ.
\end{enumerate}

\textbf{Beispiel:}  
\( X(x, y) := (2x, 2y) \), \( U := \mathbb{R}^2 \), \( x_0 := (0,0) \),  
Wähle \( \gamma(t) = t(x,y) \Rightarrow \dot{\gamma}(t) = (x,y) \),  
\[
f(x, y) = \int_0^1 X(\gamma(t)) \cdot \dot{\gamma}(t)\,dt
= \int_0^1 2t(x^2 + y^2)\,dt = (x^2 + y^2).
\]  
Dann ist \( \nabla f = (2x, 2y) = X \Rightarrow X \) ist konservativ.

\textbf{Einfach zusammenhängend:}  
\( S \subseteq \mathbb{R}^n \) ist einfach zusammenhängend, wenn jede stetige Schleife \( \gamma : S^1 \to S \) durch eine stetige Homotopie \( h : [0,1] \times S^1 \to S \) auf einen konstanten Weg deformierbar ist:
\[
h(0, y) = \gamma(y), \quad h(1, \cdot) \text{ konstant}.
\]

\textbf{Nicht-einfach zusammenhängend:}  
\( U \subset \mathbb{R}^n \) offen ist \emph{nicht} einfach zusammenhängend, wenn es ein \( C^1 \)-Vektorfeld \( X \) gibt mit  
\[
\oint_\gamma X \cdot d\gamma \neq 0
\]
für einen geschlossenen stetig differenzierbaren Weg \( \gamma : [a,b] \to U \), obwohl \( X \) die Integrabilitätsbedingung erfüllt.

\textbf{Rotation eines Vektorfeldes}

\textit{Fall \(n=2\):} Für \( X = \begin{pmatrix} X^1 \\ X^2 \end{pmatrix} : U \subset \mathbb{R}^2 \to \mathbb{R}^2 \),
\[
\operatorname{rot} X := D_1 X^2 - D_2 X^1 = \frac{\partial X^2}{\partial x^1} - \frac{\partial X^1}{\partial x^2}
\]
\textit{Bsp:} \( X = \begin{pmatrix} -y \\ x \end{pmatrix} \Rightarrow \operatorname{rot} X = 1 + 1 = 2 \)

\textit{Fall \(n=3\):} Für \( X = \begin{pmatrix} X^1 \\ X^2 \\ X^3 \end{pmatrix} : U \subset \mathbb{R}^3 \to \mathbb{R}^3 \),
\[
\vec{\operatorname{rot}} X := \nabla \times X = \begin{pmatrix}
D_2 X^3 - D_3 X^2 \\
D_3 X^1 - D_1 X^3 \\
D_1 X^2 - D_2 X^1
\end{pmatrix}
\]
\textit{Bsp:} \( X = \begin{pmatrix} -y \\ x \\ 0 \end{pmatrix} \Rightarrow \vec{\operatorname{rot}} X = \begin{pmatrix} 0 \\ 0 \\ 2 \end{pmatrix} \)

\textbf{Vertauschen partieller Ableitungen:}

\textit{Nicht immer gültig!} Ein Gegenbeispiel:
\[
f(x,y) := \begin{cases}
\frac{xy(x^2 - y^2)}{x^2 + y^2}, & (x,y)\neq(0,0) \\
0, & \text{sonst}
\end{cases}
\Rightarrow \partial_2 \partial_1 f(0,0) \ne \partial_1 \partial_2 f(0,0)
\]

\textbf{Korollar:} Falls \(f \in C^m\), dürfen partielle Ableitungen bis zur Ordnung \(m\) vertauscht werden:
\[
D_2 D_3 D_1 f = D_3 D_2 D_1 f
\]

\textit{Anwendung:} Ist \(X \in C^1\) konservativ, so gilt die Integrabilitätsbedingung:
\[
D_i X^j = D_j X^i \quad \forall\, i,j \in \{1,\dots,n\}
\]

\textbf{Taylorpolynom:} Taylorpolynom $T_{f,x_0}^m(x)$ von $f: \mathbb{R}^n \to \mathbb{R}$ um $x_0$ bis Ordnung $m$:

\[
T_{f,x_0}^m(x) := \sum_{k=0}^{m} \frac{1}{k!} \sum_{i_1,\dots,i_k=1}^{n} D_{i_k} \dots D_{i_1}f(x_0) \prod_{j=1}^{k}(x - x_0)_{i_j}
\]

\textit{Beispiel:} $f(x,y) = e^x\cos(y)$, $x_0 = (0,0)$, $m = 2$:

\[
T_{f,(0,0)}^2(x,y) = 1 + x - \frac{y^2}{2}
\]

\textbf{Taylorpolynom mit Multi-Index:}
\[
T^m_{f,x_0}(x) = \sum_{k=0}^m \sum_{\alpha \in \mathbb{N}_0^n, |\alpha| = k} \frac{1}{\alpha!} D^\alpha f(x_0) (x - x_0)^\alpha
\]

\textbf{Lemma (Multi-Index und partielle Ableitungen):}
\[
\sum_{i_1, \dots, i_k=1}^n D_{i_k} \dots D_{i_1} f(x_0) \prod_{j=1}^k v_{i_j} 
= \sum_{\alpha \in \mathbb{N}_0^n, |\alpha| = k} \binom{k}{\alpha} D^\alpha f(x_0) v^\alpha
\]

\textbf{Beispiel (Taylorpolynom bis Ordnung 2):}

Gegeben sei \( f(x_1, x_2) = e^{x_1 + x_2} \), Entwicklungspunkt \( x_0 = (0, 0) \).

Dann gilt:
\[
T^2_{f, x_0}(x) 
= f(0, 0) 
+ \nabla f(0,0) \cdot x 
+ \frac{1}{2} x^\top Hf(0,0) x
\]

Berechnung der Terme:

\[
f(0,0) = 1, \quad 
\nabla f(x) = \begin{pmatrix} \partial_{x_1} f \\ \partial_{x_2} f \end{pmatrix} 
= \begin{pmatrix} e^{x_1+x_2} \\ e^{x_1+x_2} \end{pmatrix} 
\Rightarrow \nabla f(0,0) = \begin{pmatrix} 1 \\ 1 \end{pmatrix}
\]

{\small
\[
Hf(x) = \begin{pmatrix}
\partial_{x_1}^2 f & \partial_{x_1}\partial_{x_2} f \\
\partial_{x_2}\partial_{x_1} f & \partial_{x_2}^2 f
\end{pmatrix}
= e^{x_1+x_2} \begin{pmatrix}
1 & 1 \\
1 & 1
\end{pmatrix}
\Rightarrow Hf(0,0) = \begin{pmatrix}
1 & 1 \\
1 & 1
\end{pmatrix}
\]
}

Einsetzen ergibt:
\[
T^2_{f, x_0}(x) 
= 1 + x_1 + x_2 + \frac{1}{2} \left( x_1^2 + 2x_1x_2 + x_2^2 \right)
\]

\textbf{Definition (Definitheit einer Matrix).} Eine symmetrische Matrix \( A \in \mathbb{R}^{n \times n} \) heißt
\begin{itemize}
  \item \textit{positiv definit}, falls \( x^\top A x > 0 \) für alle \( x \in \mathbb{R}^n \setminus \{0\} \),
  \item \textit{negativ definit}, falls \( x^\top A x < 0 \) für alle \( x \in \mathbb{R}^n \setminus \{0\} \),
  \item \textit{positiv semidefinit}, falls \( x^\top A x \geq 0 \) für alle \( x \in \mathbb{R}^n \),
  \item \textit{negativ semidefinit}, falls \( x^\top A x \leq 0 \) für alle \( x \in \mathbb{R}^n \),
  \item \textit{indefinit}, falls weder positiv noch negativ (semi)definit.
\end{itemize}


\textbf{Kritischer Punkt:} $x_0$ heißt \textit{kritisch}, falls $df(x_0) = 0$ bzw. $\nabla f(x_0) = 0$.

\textbf{Hesse-Matrix:} Für $f \in C^2(U)$ ist die Hesse-Matrix definiert als
\[
\text{Hess}_f(x_0) := \left(D_i D_j f(x_0)\right)_{i,j=1}^n
\]
{\small
\[
\mathbf{H}_f(x) :=
\begin{pmatrix}
\frac{\partial^2 f}{\partial x_1^2}(x) & \frac{\partial^2 f}{\partial x_1 \partial x_2}(x) & \cdots & \frac{\partial^2 f}{\partial x_1 \partial x_n}(x) \\
\frac{\partial^2 f}{\partial x_2 \partial x_1}(x) & \frac{\partial^2 f}{\partial x_2^2}(x) & \cdots & \frac{\partial^2 f}{\partial x_2 \partial x_n}(x) \\
\vdots & \vdots & \ddots & \vdots \\
\frac{\partial^2 f}{\partial x_n \partial x_1}(x) & \frac{\partial^2 f}{\partial x_n \partial x_2}(x) & \cdots & \frac{\partial^2 f}{\partial x_n^2}(x)
\end{pmatrix}
\]
}


Für $f \in C^2$ ist $\text{Hess}_f$ symmetrisch:
\[
\text{Hess}_f(x_0)_{ij} = \text{Hess}_f(x_0)_{ji}
\]

\textbf{Spezialfall $n = 1$:} Dann ist
\[
\text{Hess}_f(x_0) = f''(x_0)
\]

\textbf{Beispiel:} Sei $f(x_1, x_2) = x_1^2 + x_1 x_2 + x_2^2$, dann ist
\[
\text{Hess}_f(x_0) = 
\begin{pmatrix}
2 & 1 \\
1 & 2
\end{pmatrix}
\]

\textbf{Hinweis zur Anwendung:}
- Lokales Minimum bei $x_0$, falls $\nabla f(x_0) = 0$ und $\text{Hess}_f(x_0)$ positiv definit.
- Lokales Maximum bei $x_0$, falls $\nabla f(x_0) = 0$ und $\text{Hess}_f(x_0)$ negativ definit.

\textbf{Bsp:} \( f(x, y) = x^2 + y^2 - 4x - 6y + 13 \)

\begin{itemize}
  \item \( \nabla f = (2x - 4,\, 2y - 6) \Rightarrow (x, y) = (2, 3) \)
  \item \( \text{Hess}_f =
  \begin{pmatrix}
  2 & 0 \\
  0 & 2
  \end{pmatrix} \Rightarrow \) positiv definit
  \item \( \Rightarrow \) striktes lokales Minimum bei \( (2, 3) \)
\end{itemize}

% Umkehrsatz, implizite Funktionen___________________________________________________________________________________________________
%%%%%%
\subsection{9. Umkehrsatz, implizite Funktionen, Untermannigfaltigkeit, Tangentialraum}

% Mehrdimensionale Riemann-integration,Satz von Fubini über wiederholte Integration, Jordan-Mass, Substitutionsregel für mehrdimensionale Integrale___________________________________________________________________________________________________
%%%%%%
\subsection{10. Mehrdimensionale Riemann integration, Satz von Fubini über wiederholte Integration, Jordan-Mass, Substitutionsregel für mehrdimensionale Integrale}


% Vektorfelder und die Sätze von Green, Stokes und Gauss (WICHTIG!!!)___________________________________________________________________________________________________
%%%%%%
\subsection{11. Vektorfelder und die 
Sätze von Green, Stokes und Gauss}

\textbf{Kurvenintegral einer Funktion:}

Sei $C$ eine stückweise glatt parametrisierte Kurve $x_j : I_j \to \mathbb{R}^n$, dann ist das Kurvenintegral definiert durch
\[
\int_C f \, ds := \sum_j \int_{I_j} f(x_j(t)) \, \lVert \dot{x}_j(t) \rVert \, dt.
\]

\textbf{Heuristische Interpretation:}

\begin{itemize}
  \item $ds = \lVert \dot{x}_j(t_0) \rVert dt$ ist die Länge eines infinitesimalen Kurvenstücks.
  \item $\int_C f \, ds \approx \sum f(x_j(t_0)) \cdot \text{Länge}$ ist wie ein gewichteter Flächeninhalt.
  \item Geometrisch ist $\int_C f \, ds$ die Fläche unter dem Graphen von $f$ über der Kurve $C$.
\end{itemize}

\textbf{Satz (Green):}
Sei $U \subseteq \mathbb{R}^2$ ein beschränktes $C^1$-Gebiet und $X$ ein $C^1$-Vektorfeld auf $\overline{U}$. Dann gilt:
\[
\int_U \mathrm{rot}\, X \, dx = \int_{\partial U} X \cdot T \, ds.
\]

\textbf{Beispiel:}

Sei $X(x,y) = (-y, x)$ und $U$ die Einheitskreisscheibe. Dann ist
\[
\mathrm{rot}\, X = \frac{\partial x}{\partial x} - \frac{\partial (-y)}{\partial y} = 1 + 1 = 2.
\]
Flächenintegral:
\[
\int_U \mathrm{rot}\, X \, dx = \int_U 2 \, dx = 2 \cdot \text{Fläche}(U) = 2 \cdot \pi = 2\pi.
\]
Kurvenintegral über $\partial U$ (Einheitskreis, $T$ tangential):
\[
\int_{\partial U} X \cdot T \, ds = \int_0^{2\pi} 1 \cdot 1 \, d\varphi = 2\pi.
\]
Beide Seiten stimmen überein: Green'scher Satz verifiziert.

\textbf{Untermannigfaltigkeiten mit Rand}

Sei $M \subseteq \mathbb{R}^n$ eine $C^k$-Untermannigfaltigkeit der Dimension $d$ mit Rand.

\begin{itemize}
  \item \textbf{(i)} $M$ lokal diffeomorph zu $V \subseteq \mathbb{R}_+^d := \mathbb{R}^{d-1} \times [0,\infty)$.
  \item \textbf{(ii)} Eine Randparametrisierung $\psi: V \to \mathbb{R}^n$ mit $\psi$ injektiv und $C^k$-Immersion.
  \item \textbf{(iii)} Der Rand $\partial M$ ist wohldefiniert (unabhängig von Parametrisierung).
  \item \textbf{(iv)} $x_0 \in \partial M$ gdw. $\psi^{-1}(x_0) \in \mathbb{R}^{d-1} \times \{0\}$.
  \item \textbf{(v)} Für $M$ abgeschlossen, $d < n$: topologischer Rand $\ne$ intrinsischer Rand.
  \item \textbf{(vi)} Randdimension ist eindeutig bestimmt.
  \item \textbf{(vii)} $\psi$ darf $\partial M$ nicht doppelt überdecken.
  \item \textbf{(viii)} Bei Diffeomorphismen $f: (0,\infty) \to \mathbb{R}$ kann man alternative Parametrisierung $\chi(s,t) := (s, f(t))$ nutzen.
\end{itemize}

\textbf{Fluss durch eine Hyperfläche}

\textbf{Def.:} Sei $X$ ein Vektorfeld und $M$ eine orientierte $(n{-}1)$-dimensionale Fläche mit Normalenvektor $\nu$. Der \emph{Fluss} durch $M$ ist gegeben durch:
\[
\int_{M,\nu} X \cdot dA := \int_M X \cdot \nu \, dA.
\]

\textbf{Bemerkungen:}
\begin{itemize}
  \item (i) Heuristisch: $dA = \nu dA$, also:
  \[
  \int_{M,\nu} X \cdot dA = \sum_{z \in M} X \cdot dA.
  \]
  \item (ii) Für $n = 3$: Dies ist das (Oberflächen-)Flächenintegral von $X$ durch $M$.
  \item (iii) Motivation: In der Strömungsmechanik beschreibt der Fluss die Menge an „Material“, die pro Zeiteinheit durch $M$ strömt.
\end{itemize}

\textbf{Satz (Stokes):} \\
\[
\text{Für ein } C^1\text{-Vektorfeld } X : U \to \mathbb{R}^3, \text{ orientierte Fläche } \Sigma \subset \mathbb{R}^3: 
\]
\[
\int_{\Sigma, \nu} (\nabla \times X) \cdot dA 
= \int_{\Sigma} (\nabla \times X) \cdot \nu \, dA 
= \int_{\partial \Sigma, T} X \cdot ds 
= \int_{\partial \Sigma} X \cdot T \, ds
\]
\[
\textbf{Beispiel:} \text{ Sei } \Sigma := \{x \in S^2 \mid x_3 \geq a \}, \quad \nu(x) := x, \, a > 0. \\
\text{Gegeben:}
\]
\[
X(x) := \frac{1}{2}(-x_2, x_1, 0), \quad T(x) := \frac{1}{\sqrt{1 - a^2}}(-x_2, x_1, 0)
\]

\text{Dann:}
\[
\nabla \times X = (0, 0, 1) = e_3
\]

$\text{Fluss durch } \Sigma: $
\[
\int_{\Sigma, \nu} e_3 \cdot dA 
= \int_{\partial \Sigma} X \cdot T \, ds 
= \int_{S^1} \frac{1}{\sqrt{1-a^2}} ((-x_2)^2 + x_1^2) \, ds 
= \frac{1}{\sqrt{1 - a^2}} \int_{S^1} (x_1^2 + x_2^2) \, ds
\]

$\text{Auf Kreisradius } \sqrt{1 - a^2}: $
\[
= \frac{1}{\sqrt{1 - a^2}} (1 - a^2) \cdot 2\pi\sqrt{1 - a^2}
= 2\pi (1 - a^2)
\]

\textbf{Definition: Koorientierung eines Randes}

Sei $U \subset \mathbb{R}^n$ ein $C^1$-Gebiet und $g \in C^1(U, \mathbb{R})$ mit $\partial U = \{x \in U : g(x) = 0\}$, $\nabla g(x_0) \neq 0$.  
Dann ist die nach außen zeigende \textit{koorientierte} Einheitsnormalenabbildung
\[
\nu: \partial U \to \mathbb{R}^n, \quad \nu(x) := \frac{\nabla g(x)}{\|\nabla g(x)\|}.
\]

\textbf{Eigenschaft:}  
Die Koorientierung definiert eine Orientierung für $\partial U$ konsistent mit $U$.

\textbf{Satz von Gauß (Divergenzsatz)}

Sei $U \subseteq \mathbb{R}^n$ ein beschränktes $C^1$-Gebiet und $X \in C^1(\overline{U}, \mathbb{R}^n)$. Dann gilt:

\[
\int_U \operatorname{div} X \, dx = \int_{\partial U, \nu} X \cdot dA = \int_{\partial U} X \cdot \nu \, dA
\]

Dabei ist $\nu$ das nach außen weisende Einheitsnormalenfeld auf $\partial U$.

\textbf{Beispiel ($n=1$)}
Für $f = X^1$ und $U = (a,b)$:

\[
\int_a^b f'(x)\, dx = f(b) - f(a)
\]

\textbf{Begründung:} Mit $X = f$ ist $\operatorname{div} X = f'$ und
\[
\int_U \operatorname{div} X \, dx = \int_{\partial U} f \cdot \nu = f(b)\cdot 1 + f(a)\cdot (-1) = f(b) - f(a)
\]

\textbf{Folgerung aus Gauß (Satz 11.44):}

Aus dem Gaußschen Integralsatz folgt:
\[
\int_U \operatorname{rot} X \, dx 
= \int_U \nabla \cdot Y \, dx 
= \int_{\partial U} Y \cdot \nu \, dA 
= \int_{\partial U} \left( X^2 T^2 - X^1 (-T^1) \right) \, ds \quad \text{(wegen Beispiel 11.39)}
= \int_{\partial U} X \cdot T \, ds.
\]


% Weiteres___________________________________________________________________________________________________
%%%%%%
\subsection{12. Zusatz (Integraltabellen, Bilder etc.}